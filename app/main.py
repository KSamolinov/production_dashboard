from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from pydantic import BaseModel
from datetime import datetime, timedelta
import pandas as pd
import graphs
import data_collection
import io
import os
import logging

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

# -----------------------------------------------------------------------------
# App & logging
# -----------------------------------------------------------------------------
app = FastAPI()
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

scheduler = BackgroundScheduler(timezone="Europe/Moscow")

# -----------------------------------------------------------------------------
# Paths & templates
# -----------------------------------------------------------------------------
# --- Paths & templates ---
BASE_DIR = Path(__file__).resolve().parent

print(BASE_DIR)

CARDS_DIR = BASE_DIR / "data" / "cards prod"
STATS_DIR = BASE_DIR / "data" / "table prod"

print(CARDS_DIR)
print(STATS_DIR)

LOCAL_OUTPUT_PATH = Path(os.getenv("LOCAL_OUTPUT_PATH", BASE_DIR / "data"))
LOCAL_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

DATA_PATH = Path(BASE_DIR, "data", "card_full_data.csv")

print(DATA_PATH)

templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))

def _check_dir(p: Path, must_write: bool = False) -> dict:
    info = {"path": str(p), "exists": p.exists(), "is_dir": p.is_dir(), "readable": False, "writable": False, "sample": []}
    try:
        if p.exists() and p.is_dir():
            # Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ð°
            info["sample"] = sorted([f.name for f in p.iterdir()])[:5]
            # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ
            for _ in p.iterdir():
                info["readable"] = True
                break
            # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
            if must_write:
                testfile = p / ".write_test"
                try:
                    testfile.write_text("ok", encoding="utf-8")
                    testfile.unlink(missing_ok=True)
                    info["writable"] = True
                except Exception:
                    info["writable"] = False
        return info
    except Exception:
        return info

def verify_paths(strict: bool = False) -> dict:
    cards = _check_dir(CARDS_DIR, must_write=False)
    table = _check_dir(STATS_DIR, must_write=False)
    outd  = _check_dir(LOCAL_OUTPUT_PATH, must_write=True)
    status = {
        "cards": cards,
        "table": table,
        "out": outd,
        "ok": (
            cards["exists"] and cards["is_dir"] and cards["readable"] and
            table["exists"] and table["is_dir"] and table["readable"] and
            outd["exists"] and outd["is_dir"] and (outd["writable"] or not strict)
        )
    }
    return status


# Ð¡Ñ‚Ð°Ñ€Ñ‚Ð¾Ð²Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°: Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) Ð±Ð»Ð¾ÐºÐ¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‚ Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»Ð½Ð¾Ð¼ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð¿ÑƒÑ‚ÐµÐ¹
@app.on_event("startup")
async def _startup_check():
    status = verify_paths(strict=False)
    # Ð•ÑÐ»Ð¸ ÑÐµÑ‚ÐµÐ²Ñ‹Ðµ Ð¿ÑƒÑ‚Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ â€” Ð¼Ð¾Ð¶Ð½Ð¾ Ð»Ð¸Ð±Ð¾ ÑƒÐ¿Ð°ÑÑ‚ÑŒ, Ð»Ð¸Ð±Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð»Ð¾Ð³Ð½ÑƒÑ‚ÑŒ Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð² Ð´ÐµÐ³Ñ€Ð°Ð´Ð°Ñ†Ð¸Ð¸
    if not status["ok"]:
        # Ð²Ñ‹Ð±ÐµÑ€Ð¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ:
        # 1) Ð–Ñ‘ÑÑ‚ÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ ÑÑ‚Ð°Ñ€Ñ‚ÑƒÐµÐ¼
        #   raise RuntimeError(f"Paths not available: {status}")
        # 2) ÐœÑÐ³ÐºÐ¾: Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð»Ð¾Ð³ (Ð¾ÑÑ‚Ð°Ð²Ð»ÑŽ Ð¼ÑÐ³ÐºÐ¸Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
        print("[WARN] Some paths are not available:", status)

# Healthcheck Ð´Ð»Ñ Docker
@app.get("/healthz")
def healthz():
    status = verify_paths(strict=True)
    code = 200 if status["ok"] else 503
    return JSONResponse(content=status, status_code=code)

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def update_data_job():
    """Ð¤Ð¾Ð½Ð¾Ð²Ð¾Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ¾Ð¼)."""
    try:
        logging.info("ðŸ”„ Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        data_collection.main()
        logging.info("âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾")
    except Exception as e:
        logging.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")

def load_data() -> pd.DataFrame:
    print(f"[DEBUG] DATA_PATH = {DATA_PATH.resolve()}")
    print(f"[DEBUG] Exists? {DATA_PATH.exists()}")
    # ÐµÑÐ»Ð¸ Ñ„Ð°Ð¹Ð»Ð° Ð½ÐµÑ‚ â€” ÑÐ¾Ð±ÐµÑ€Ñ‘Ð¼
    if not DATA_PATH.exists():
        data_collection.main()

    if not DATA_PATH.exists():
        raise FileNotFoundError(f"Ð¤Ð°Ð¹Ð» Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {DATA_PATH}")

    df = pd.read_csv(DATA_PATH)
    df["Ð”Ð°Ñ‚Ð°"] = pd.to_datetime(df["Ð”Ð°Ñ‚Ð°"], errors="coerce")
    return df


    # ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼ Ð´Ð°Ñ‚Ñ‹
    df["Ð”Ð°Ñ‚Ð°"] = pd.to_datetime(df["Ð”Ð°Ñ‚Ð°"], errors="coerce")
    return df

def get_default_period() -> list[str]:
    """ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 14 Ð´Ð½ÐµÐ¹, Ð² ISO (YYYY-MM-DD)."""
    today = datetime.today().date()
    start = today - timedelta(days=13)
    return [start.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d")]

DEFAULT_PERIOD = get_default_period()

def to_display(d: str) -> str:
    return pd.to_datetime(d).strftime("%d.%m.%y")

def parse_period(start: str, end: str) -> list[pd.Timestamp]:
    s = pd.to_datetime(start, errors="coerce")
    e = pd.to_datetime(end, errors="coerce")

    if pd.isna(s) or pd.isna(e):
        s, e = pd.to_datetime(DEFAULT_PERIOD[0]), pd.to_datetime(DEFAULT_PERIOD[1])
    if s > e:
        s, e = e, s
    return [s, e]

# -----------------------------------------------------------------------------
# Models
# -----------------------------------------------------------------------------
class KPIResponse(BaseModel):
    total_cards: int
    total_defects: int
    defect_percent: float
    total_money: float

# -----------------------------------------------------------------------------
# Endpoints: ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
# -----------------------------------------------------------------------------
@app.get("/nomenclature")
async def get_nomenclature():
    df = load_data()
    nomen_list = sorted(df["ÐÐ¾Ð¼ÐµÐ½ÐºÐ»Ð°Ñ‚ÑƒÑ€Ð°"].dropna().astype(str).unique().tolist())
    return JSONResponse(nomen_list)

@app.get("/places")
async def get_places():
    df = load_data()
    places = sorted(df["Ð£Ñ‡Ð°ÑÑ‚Ð¾Ðº"].dropna().astype(str).unique().tolist())
    return JSONResponse(places)

# -----------------------------------------------------------------------------
# Index (ÑˆÐ°Ð±Ð»Ð¾Ð½)
# -----------------------------------------------------------------------------
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    df = load_data()
    start_iso, end_iso = DEFAULT_PERIOD
    total_cards, total_defects, defect_percent, total_money = graphs.get_kpis(
        df, parse_period(start_iso, end_iso)
    )
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "start_iso": start_iso,
            "end_iso": end_iso,
            "start_display": to_display(start_iso),
            "end_display": to_display(end_iso),
            "total_cards": f'{total_cards:,.2f}'.replace(",", " ").replace(".00", "") + " ÑˆÑ‚",
            "total_defects": f'{total_defects:,.2f}'.replace(",", " ").replace(".00", "") + " ÑˆÑ‚",
            "defect_percent": defect_percent,
            "total_money": f"{total_money:,.2f}".replace(",", " ").replace(".00", "") + " â‚½",
        },
    )

# -----------------------------------------------------------------------------
# Endpoints: Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸
# -----------------------------------------------------------------------------
@app.get("/plots/line_total")
async def plot_line_total(
    start: str = DEFAULT_PERIOD[0],
    end: str = DEFAULT_PERIOD[1],
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)
    img_bytes = graphs.plot_line_total(filtered, parse_period(start, end))
    return StreamingResponse(io.BytesIO(img_bytes), media_type="image/png")

@app.get("/plots/line_defects")
async def plot_line_defects(
    start: str = DEFAULT_PERIOD[0],
    end: str = DEFAULT_PERIOD[1],
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)
    img_bytes = graphs.plot_line_defects(filtered, parse_period(start, end))
    return StreamingResponse(io.BytesIO(img_bytes), media_type="image/png")

@app.get("/plots/bar_defects")
async def bar_defects(
    start: str = DEFAULT_PERIOD[0],
    end: str = DEFAULT_PERIOD[1],
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)
    img_bytes = graphs.bar_plot_defects(filtered, parse_period(start, end))
    return StreamingResponse(io.BytesIO(img_bytes), media_type="image/png")

@app.get("/plots/pie_defects")
async def pie_defects(
    start: str = DEFAULT_PERIOD[0],
    end: str = DEFAULT_PERIOD[1],
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)
    img_bytes = graphs.pie_plot_defects(filtered, parse_period(start, end))
    return StreamingResponse(io.BytesIO(img_bytes), media_type="image/png")

# -----------------------------------------------------------------------------
# Endpoints: Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸ KPI
# -----------------------------------------------------------------------------
@app.get("/tables/defects")
async def defects_table(
    start: str,
    end: str,
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)

    start_dt, end_dt = parse_period(start, end)
    period_df = filtered[(filtered["Ð”Ð°Ñ‚Ð°"] >= start_dt) & (filtered["Ð”Ð°Ñ‚Ð°"] <= end_dt)]

    defects_df = graphs.defects_data(period_df)

    if defects_df.empty:
        return JSONResponse([{"defect": "ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…", "sum": None}])

    safe_df = defects_df.copy()
    # Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ, None Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼
    safe_df["sum"] = safe_df["sum"].apply(
        lambda x: 0
        if isinstance(x, (int, float)) and (pd.isna(x) or x in [float("inf"), float("-inf")])
        else x
    )
    return JSONResponse(safe_df.to_dict(orient="records"))

@app.get("/kpi", response_model=KPIResponse)
async def get_kpi_data(
    start: str,
    end: str,
    nomen: str | None = None,
    place: str | None = None,
):
    df = load_data()
    filtered = graphs.prepare_data(df, nomen=nomen, place=place)
    total_cards, total_defects, defect_percent, total_money = graphs.get_kpis(
        filtered, parse_period(start, end)
    )
    return KPIResponse(
        total_cards=total_cards,
        total_defects=total_defects,
        defect_percent=defect_percent,
        total_money=total_money,
    )

# -----------------------------------------------------------------------------
# Manual trigger (Ð¿Ð¾ Ð¶ÐµÐ»Ð°Ð½Ð¸ÑŽ: Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
# -----------------------------------------------------------------------------
@app.post("/update")
async def manual_update():
    update_data_job()
    return {"status": "ok", "message": "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ"}

# -----------------------------------------------------------------------------
# Scheduler lifecycle (startup/shutdown)
# -----------------------------------------------------------------------------
@app.on_event("startup")
async def startup_event():
    """Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ: Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ€Ð°Ð· Ð² Ñ‡Ð°Ñ."""
    minutes = int(os.getenv("REFRESH_INTERVAL_MINUTES", "60"))
    try:
        if not scheduler.running:
            scheduler.add_job(
                update_data_job,
                IntervalTrigger(minutes=minutes),
                id="update_data_job",
                replace_existing=True,
            )
            scheduler.start()
            logging.info(f"â° ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½: ÐºÐ°Ð¶Ð´Ñ‹Ðµ {minutes} Ð¼Ð¸Ð½.")
    except Exception as e:
        logging.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐµ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ°: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ: ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð²Ñ‹ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº."""
    try:
        if scheduler.running:
            scheduler.shutdown(wait=False)
            logging.info("ðŸ›‘ ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½")
    except Exception as e:
        logging.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ°: {e}")

